cite2vec : Citation-Driven Document Exploration via Word
Embeddings

用户基于usage是具有潜力的可视化的挖掘方向

kernel:
cite2vec

approach:
映射 能代表文档 words 到二维空间

将能代表文档的词语 映射 二位空间

然后将 可以表用户 文档投影到相同的空间

用可以表明文档中的单词 manner of usage  这样保留了
document-document usage
同时用户就可以交互式的进行

用户最终的是可以 更改投影,这个时候

随着documents positioned and encoded
with the word-composition

在 2D 的映射 空间 中 融入了 类比 analogy 的 概念

2D 的 concept 空间 代表 了现在 类比的 结论

2D 空间 呈现 着 类比的 概念

所有形式的交互动态过滤着 文档 和 更改 文档的 位置 -- 这样的
目的是反映出用户对于这个 2D 空间的定义

这个时候需要模型 来 集成 以及 生成 更多的 words

需求:
文档能被忠实的 代表
产生的word 是有意义的
人机交互要自然

word2vec model .

originally designed to learn embedding spaces
保留有 word semantics .

将 document 视为 word 是通过 citations , 同时 允许 我们 将
word 与 document 嵌入 (学习) 到 同一个空间

word2vec have the linear struct

t-SNE 提供了一个 非常 具有效果的 降低维度 处理方法
每一个 document 在 high dimensional is a point .
在 map 到 high dimensionality 的时候 , term 与 frequence 成
比例


原始的 高维 削减 是指的是 multidimensional scaling 和
self-organizing map.

t-SNE 是最近有良好 效果的.
t-SNE 是一种重尾分布的模型

前期的方式是 用户 手动 指定 一些文档

Least Squares Projection approach .

Laplacian regularization
之后的技术集成了这些方法

hierarchical sampling
locally-afine
radia basis function regression 径向基础

Topic Models
Latent Dirichlet Allocation
潜在 狄利克雷 分配

cite2vec 模型 可以 使用户 输入 任意的语句 而不是已经有的
topic model distributions

虽然通过 更改 LDA 的参数也是可以做到 ,
但是那样的方式 本质 是 通过 直接 更改 文档之间 的相关性
比如相关系数等等 , 而不是通过 model 进行的 更改.


our model is via
the citation context

EigenFactor metricx use the eigenvector centrality to rank
ournals


Citespace II allow the user to get info through the citation
network graphs under different time windows

citation contexts themselves Metro map 可视化 的 检测 引用网
络中的依赖关系 , 在 windows 中呈现出来.

depict 描绘
为了使 读者 有 更好的理解脉络 . Co-citation analysis is performed to depic
hierachical relationship between papers .

这些方法都没有实际 引用 文档里面的语句作为 可视化的呈现部分.
除过 CiteRivers system . 这个模型是 主要 使用 以 conjunction
with the actual document content to visualize various aspect
.

cite2Vec model 正式开始 :

Kernel for :
T2 Steer document exploration via usage
语言和语法用户 所使用的 与文章
互动
T3 Compare document usage .

A 收集 信息 分析引用 关系 对引用进行标注 独立的 PID
B WORD-DOCUMENT EMBEDDING 保留关系
在学习的过程 视 每个文档为一个独立的单词
继承了线性结构
C 投影 文档 相关的 词语 从 高位空间 到 2D
D project the document 文档 与 word 的 relevant 保留 ,
文档与 文档 间的 相似 保留.
利用 word 的 linear structure 去 embedding . 这样使得 users
可以 根据 自己的兴趣 更改 文档的二位 投影 2T

E F  过滤 不相关 的
然后 将 相关的移动 到 合适的 位置 .
并且 respecting document structure .

数据很大 CV
共计 12223 papers

Cw content window determines how many surrounding context
vectors
k determines the how man negative samples to draw per word

The first term encouage the word-content co-occur o have
high likelihood . which result in closing in the embedding.

skip-gram model

The second term ensourage words to be sufficiently distinct
from randoml drawn content . 这个是用来是那些 词语 被 原理
用户 任意填充 的 词语的.
看样子应该是达到的是过滤的作用.

citation as words

向量空间模型在自然语言处理领域中有着漫长且丰富的历史
，不过几乎所有利用这一模型的方法都依赖于 分布式假设
，其核心思想为出现于上下文情景中的词汇都有相类似的语义。

Word2vec
是一种可以进行高效率词嵌套学习的预测模型
。其两种变体分别为
：连续词袋模型（CBOW）及Skip-Gram模型。
从算法角度看，这两种方法非常相似，其区别为CBOW根据源词上下文
词汇
（'the cat sits on the'）来预测目标词汇（例如，‘mat’），
而Skip-Gram模型做法相反，它通过目标词汇来预测源词汇。
Skip-Gram模型采取CBOW的逆过程的动机在于：
CBOW算法对于很多分布式信息进行了平滑处理（例如将一整段上下文
信息视为一个单一观察量）。
很多情况下，对于小型的数据集，这一处理是有帮助的。
相形之下，Skip-Gram模型将每个“上下文-目标词汇”的组合视为一个
新观察量，这种做法在大型数据集中会更为有效。

神经概率化语言模型通常使用极大似然法
 (ML) 进行训练
，其中通过 softmax function 来最大化当提供前一个单词
h (代表 "history")，
后一个单词的概率  (代表 "target")，

他们实现 的 cite2vec 模型中

Cw 的取值他们设定为 30 +- 10

他们在 动态的调整 Cw 因为 引用周围的词语 更具有 相关性
通过 论文 30 丢掉 频度 不高 的词语

embedding properities

perform k-means k = 300
for each cluster 5 word

4.4 word projections
P1 the word can cover general  themes found within document
P2 word is semantically distinct from one another
P3 is in a mutil-scale manner

首先 取 最远 点

算出 最大 的远距离 也就是 高纬度的 cos 最大化.

然后对于 每个文档 取得 word 按照 如下 约束
如果最近的词已经在之前或之后被采样
到已经采样的字的字符串编辑距离在阈值内
丢弃 . 下一个

我们表明 采样 结果是 Ws 对 s 的采样
词是在语义上不同并且表示文档中的多样性

下一步 :
Barnes-Hut t-SNE

推动不同
物体远离投影
空间，更好地区分相似的词，最终令人满意
我们的多尺度抽样标准

也就是那张图片表示的那个样子.

4.5 Document Project

根据 用户 的 交互 我们 确立了 文档投影

The linear structure in the embedding permits us to
represent a document as a summation of word vectors

As well as the document vectors

使 word 呈现在 用户自定义的Set 中

任意短语，文档或文档短语类比

概念集合 C  里 所有 呈现 自己所具有的意义

投影 document

word project 保持 不变

每个文档都投影接近其最佳匹配的字词组成 - 概念有效地引导文档
投影。

文档嵌入空间的几何结构应尽可能保留。 这试图保存文档 - 文档关
系。

issue 1.
不同单词 可能 由 不同 概念 取决于 文档
不能 完全依靠 文字来 保留 文件 投影 关系 同时 一个 文档 也由
不同 概念 组成

这个 是 与
Least Squares Projection apporach 相似 的 最小二乘
